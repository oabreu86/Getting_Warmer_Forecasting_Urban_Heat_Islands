{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSC Project 2021 Land Sat Temp Machine Learning: Dask on the Midway Cluster (oabreu_sjaisha_gdmorrison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chicago's landsat8 raw image bands dataset (2013-2020) for this notebook are available in the `/project2/macs30123/project_landsat/` directory on Midway2.\n",
    "\n",
    "To run this notebook, you should log in to the Midway Cluster [via ThinLinc](https://midway2.rcc.uchicago.edu/main/). After you're logged in, run the following lines of code in a terminal window from a login node (to load the Python module we've been working with in the class and then install a couple of additional packages we'll be using):\n",
    "```\n",
    "module load python/anaconda-2019.03\n",
    "pip install --user \"dask[complete]\" dask-jobqueue dask-ml --upgrade\n",
    "```\n",
    "Once you have installed these packages, start up a Jupyter Notebook from the login node (you'll be requesting resources via `dask-jobqueue`'s SLURM functions, on which you will run your code):\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching a Dask Cluster via SLURM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To request Midway Cluster resources for our Dask Cluster, we have installed [dask-jobqueue](https://jobqueue.dask.org/en/latest/), which can be used to deploy Dask via common job queuing systems like SLURM (used in the Midway Cluster). Furthermore, it allows us to perform these interactions with SLURM an interactive context within a Jupyter Notebook.\n",
    "\n",
    "Specifically, we'll be using the SLURMCluster function to request resources, using many of the same key words that we provide when write an `sbatch` script or provide arguments for `sinteractive` jobs. For instance, here, we're requesting 10 `broadwl` cores with 40GB of memory that are connected via an `ib0` (InfiniBand) interconnect (which we'll have available to us for 1 hour):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTIVATING CLIENT ON MIDWAY\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "# Compose SLURM script\n",
    "cluster = SLURMCluster(queue='broadwl', cores=12, memory='40GB', \n",
    "                       processes=10, walltime='06:00:00', interface='ib0',\n",
    "                       job_extra=['--account=macs30123']\n",
    "                      )\n",
    "\n",
    "# Request resources\n",
    "cluster.scale(jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to wait a bit for our resources to be provisioned, but we can check on the progress of our resources via `squeue` as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! squeue -u oabreu\n",
    "! squeue -u syedajaisha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our resources have been provisioned, we need to tell Dask that it should use them to run its computations. We can do this, by passing `cluster` into our `dask.distributed` client object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/distributed/node.py:151: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 55777 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:55778</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:55777/status' target='_blank'>http://127.0.0.1:55777/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:55778' processes=4 threads=4, memory=17.18 GB>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ACTIVATING CLIENT ON LOCAL MACHINE\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all we need to do! If we want to see an interactive representation of what our workers are doing at any given time, we can click the link above, as in all the other Dask setups. \n",
    "\n",
    "We're now ready to start using Dask. Let's start by reading our Landsat Images Dataframe composed of all the features we extracted/engineered from the band data into a Dask DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RUN THIS ONE IF ON MIDWAY WITH THE NAME OF THE FILE;ONCE WE GET FILE ON MIDWAY, EDIT THE PATH BELOW\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv('/project2/macs30123/project_landsat/___.csv')\n",
    "display(df.head())\n",
    "display(df.dtypes)\n",
    "display(df.describe().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>45</td>\n",
       "      <td>191</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-02 08:00:00</td>\n",
       "      <td>54</td>\n",
       "      <td>155</td>\n",
       "      <td>2015</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-08-31 16:00:00</td>\n",
       "      <td>71</td>\n",
       "      <td>134</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-31 00:00:00</td>\n",
       "      <td>94</td>\n",
       "      <td>137</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01 00:00:00</td>\n",
       "      <td>39</td>\n",
       "      <td>169</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index   X    Y  Year  month\n",
       "0 2015-01-01 00:00:00  45  191  2015      1\n",
       "1 2015-05-02 08:00:00  54  155  2015      5\n",
       "2 2015-08-31 16:00:00  71  134  2015      8\n",
       "3 2015-12-31 00:00:00  94  137  2015     12\n",
       "4 2016-01-01 00:00:00  39  169  2016      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "index    datetime64[ns]\n",
       "X                 int64\n",
       "Y                 int64\n",
       "Year              int64\n",
       "month             int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FOR TESTING ONLY; DELETE ONCE TRUE DATAFRAME CODE WORKS\n",
    "def create_test_df():\n",
    "    ts_2015 = pd.date_range('2015-01-01', '2015-12-31', periods=4).to_series()\n",
    "    ts_2016 = pd.date_range('2016-01-01', '2016-12-31', periods=12).to_series()\n",
    "    ts_2017 = pd.date_range('2017-01-01', '2017-12-31', periods=6).to_series()\n",
    "    ts_2018 = pd.date_range('2018-01-01', '2018-12-31', periods=8).to_series()\n",
    "    ts_2019 = pd.date_range('2019-01-01', '2019-12-31', periods=24).to_series()\n",
    "    ts_2020 = pd.date_range('2020-01-01', '2020-12-31', periods=30).to_series()\n",
    "    ts_all = pd.concat([ts_2015, ts_2016, ts_2017, ts_2018, ts_2019, ts_2020])\n",
    "    df = pd.DataFrame({'X': np.random.randint(0, 100, size=ts_all.shape), \n",
    "                   'Y': np.random.randint(100, 200, size=ts_all.shape)},\n",
    "                 index=ts_all)\n",
    "    df['Year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "df = create_test_df()\n",
    "display(df.head())\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score, \\\n",
    "                            precision_score, recall_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOES NOT WORK DUE TO DEPENDENCY ISSUE LOCALLY, TRY RUNNING IN MIDWAY INSTANC\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe_normalize(df, scaler=None, outputinc=False, outputcol=None):\n",
    "    '''\n",
    "    Normalizes dataframe (adapted from Nick Feamster's normalize function)\n",
    "    Inputs:\n",
    "        df (Pandas Dataframe)\n",
    "        scaler (Scaler) :If scaler is not none, use given scaler's means and sds\n",
    "                         to normalize (input for test set case); else, set \n",
    "                         scaler in function\n",
    "        outputinc (bool): If output is included, set aside to ensure it does not\n",
    "                          get normalized, default False\n",
    "        outputcol (str): If output is included, name of output column, default\n",
    "                         None\n",
    "    Returns tuple of:\n",
    "        Normalized DataFrame and scaler used to normalize DataFrame\n",
    "    '''\n",
    "    columns = df.columns\n",
    "    if outputinc:\n",
    "        outcomes = df.loc[:,outputcol]\n",
    "        df = pd.DataFrame(df.drop(outputcol, axis=1))\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        normalized_features = scaler.fit_transform(df) \n",
    "    else:\n",
    "        normalized_features = scaler.transform(df)\n",
    "\n",
    "    normalized_df = pd.DataFrame(normalized_features)\n",
    "    if outputinc:\n",
    "        normalized_df[outputcol] = outcomes.tolist()\n",
    "\n",
    "    normalized_df.index=df.index\n",
    "    normalized_df.columns= columns\n",
    "\n",
    "    return normalized_df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(df, split = default_split, ycol = default_ycol):\n",
    "    k = len(split)\n",
    "\n",
    "    df_train = [pd.DataFrame(columns = list(df.columns))]*k\n",
    "    df_val = [pd.DataFrame(columns = list(df.columns))]*k\n",
    "    df_test = df[df[\"Year\"] == test_year]\n",
    "\n",
    "    for i in range(k):\n",
    "        for train_yr in split[i][0]:\n",
    "            df_train[i] = df_train[i].append(df[df[\"Year\"] == train_yr])\n",
    "        df_val[i] = df_val[i].append(df[df[\"Year\"] == split[i][1]])\n",
    "\n",
    "    df_train_y = [None]*k\n",
    "    df_train_x = [None]*k\n",
    "    df_val_y = [None]*k\n",
    "    df_val_x = [None]*k\n",
    "\n",
    "    for i in range(k):\n",
    "        df_train_y[i] = df_train[i][ycol]\n",
    "        df_train_x[i] = df_train[i].drop(columns = [ycol, \"Year\"])\n",
    "        df_val_y[i] = df_val[i][ycol]\n",
    "        df_val_x[i] = df_val[i].drop(columns = [ycol, \"Year\"])\n",
    "        df_test_y = df_test[ycol]\n",
    "        df_test_x = df_test.drop(columns = [ycol, \"Year\"])\n",
    "\n",
    "    return df_train_y, df_train_x, df_val_y, df_val_x, df_test_y, df_test_x\n",
    "\n",
    "def normalize(df_train_x, df_val_x, df_test_x):\n",
    "    k = len(df_train_x)\n",
    "    train_norm = []\n",
    "    valid_norm = []\n",
    "    for n in range(k):\n",
    "        df = pd.concat((df_train_x[n], df_val_x[n]))\n",
    "        df_norm, scaler = pipe_normalize.normalize(df)\n",
    "        tr_norm = df_norm.loc[df_train_x[n].index,:]\n",
    "        val_norm = df_norm.loc[df_val_x[n].index,:]\n",
    "        train_norm.append(tr_norm)\n",
    "        valid_norm.append(val_norm)\n",
    "    te_norm, _ = pipe_normalize.normalize(df_test_x, scaler=scaler)\n",
    "    test_norm = te_norm\n",
    "    return train_norm, valid_norm, test_norm\n",
    "\n",
    "def grid_search_time_series_cv(df_train_y, df_train_x, df_val_y, df_val_x,\n",
    "                               models, p_grid, ret_int_results = False, print = False):\n",
    "    k = len(df_train_y)\n",
    "    val_results = [pd.DataFrame(columns = [\"Model\", \"Params\", \"RMSE\", \"MAE\", \"R^2\"])]*k\n",
    "\n",
    "    for i in range(k):\n",
    "        for model_key in models.keys():\n",
    "            for params in p_grid[model_key]:\n",
    "                if print == True:\n",
    "                    print(\"Training model:\", model_key, \"|\", params)\n",
    "                model = models[model_key]\n",
    "                model.set_params(**params)\n",
    "                fitted_model = model.fit(df_train_x[i], df_train_y[i])\n",
    "                test_predictions = fitted_model.predict(df_val_x[i])\n",
    "                rmse = mean_squared_error(df_val_y[i], test_predictions, squared = False)\n",
    "                mae = mean_absolute_error(df_val_y[i], test_predictions)\n",
    "                r2 = r2_score(df_val_y[i], test_predictions)\n",
    "                val_results[i] = val_results[i].append(pd.DataFrame([[model_key, params, rmse, mae, r2]],\n",
    "                                                       columns = [\"Model\", \"Params\", \"RMSE\", \"MAE\", \"R^2\"]))\n",
    "\n",
    "    avg_val_results = pd.DataFrame(columns = [\"Model\", \"Params\", \"RMSE\", \"RMSE std dev\", \"MAE\", \"R^2\"])\n",
    "    avg_val_results[\"Model\"] = val_results[0][\"Model\"]\n",
    "    avg_val_results[\"Params\"] = val_results[0][\"Params\"]\n",
    "    avg_val_results[\"RMSE\"] = [0]*len(val_results[0])\n",
    "    avg_val_results[\"RMSE std dev\"] = [0]*len(val_results[0])\n",
    "    avg_val_results[\"MAE\"] = [0]*len(val_results[0])\n",
    "    avg_val_results[\"R^2\"] = [0]*len(val_results[0])\n",
    "    for i in range(k):\n",
    "        avg_val_results[\"RMSE\"] += val_results[i][\"RMSE\"]/k\n",
    "        avg_val_results[\"MAE\"] += val_results[i][\"MAE\"]/k\n",
    "        avg_val_results[\"R^2\"] += val_results[i][\"R^2\"]/k\n",
    "    avg_val_results = avg_val_results.reset_index().drop(columns = [\"index\"])\n",
    "    l0 = list(val_results[0][\"RMSE\"])\n",
    "    l1 = list(val_results[1][\"RMSE\"])\n",
    "    l2 = list(val_results[0][\"RMSE\"])\n",
    "    for i in range(len(avg_val_results)):\n",
    "        avg_val_results.iloc[i, [3]] = np.std([l0[i], l1[i], l2[i]])\n",
    "\n",
    "    if ret_int_results == True:\n",
    "        return avg_val_results, val_results\n",
    "    else:\n",
    "        return avg_val_results\n",
    "\n",
    "def select_best_model(avg_val_results, selection_param = default_selection_param):\n",
    "    best_model = avg_val_results[avg_val_results[selection_param] == avg_val_results[selection_param].min()].iloc[0]\n",
    "    return best_model\n",
    "\n",
    "def select_model(avg_val_results, row):\n",
    "    chosen_model = avg_val_results.iloc[row]\n",
    "    return chosen_model\n",
    "\n",
    "def test_model(df_train_y, df_train_x, df_val_y, df_val_x, df_test_y, df_test_x,\n",
    "               chosen_model, models):\n",
    "    k = len(df_train_y)\n",
    "    model = models[chosen_model[\"Model\"]]\n",
    "    model.set_params(**chosen_model[\"Params\"])\n",
    "\n",
    "    df_tv_x = pd.concat([df_train_x[k-1], df_val_x[k-1]])\n",
    "    df_tv_y = pd.concat([df_train_y[k-1], df_val_y[k-1]])\n",
    "\n",
    "    fitted_model = model.fit(df_tv_x, df_tv_y)\n",
    "    test_predictions = fitted_model.predict(df_test_x)\n",
    "    rmse = mean_squared_error(df_test_y, test_predictions, squared = False)\n",
    "    mae = mean_absolute_error(df_test_y, test_predictions)\n",
    "    r2 = r2_score(df_test_y, test_predictions)\n",
    "    test_results = {\"RMSE\" : rmse, \"MAE\" : mae, \"r^2\" :r2}\n",
    "    return test_results\n",
    "\n",
    "def choose_and_test_model(df, models, p_grid, n_splits = 3, ycol = default_ycol, selection_param = default_selection_param):\n",
    "    df_train_y, df_train_x, df_val_y, df_val_x, df_test_y, df_test_x = train_val_test_split(df, split, ycol)\n",
    "    df_train_x, df_val_x, df_test_x = normalize(df_train_x, df_val_x, df_test_x)\n",
    "    avg_val_results = grid_search_time_series_cv(df_train_y, df_train_x, df_val_y, df_val_x, models, p_grid)\n",
    "    best = select_best_model(avg_val_results, selection_param)\n",
    "    test_results = test_model(df_train_y, df_train_x, df_val_y, df_val_x, df_test_y, df_test_x, best, models)\n",
    "    return test_results, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_split = {0: [[2015], 2016],\n",
    "                 1: [[2015, 2016], 2017],\n",
    "                 2: [[2015, 2016, 2017], 2018],\n",
    "                 3: [[2015, 2016, 2017, 2018], 2019],\n",
    "                 4: [[2015, 2016, 2017, 2018, 2019], 2020]}\n",
    "                 \n",
    "test_year = 2020\n",
    "default_ycol = \"Y\"\n",
    "default_selection_param = \"RMSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"LinearRegression\" : LinearRegression(),\n",
    "          \"Ridge\" : Ridge(),\n",
    "          \"Lasso\" : Lasso(),\n",
    "          \"ElasticNet\" : ElasticNet()}\n",
    "\n",
    "p_grid = {\"LinearRegression\" : [{}],\n",
    "          \"Ridge\" : [{\"alpha\" : x} for x in [.1, .5, 1, 5, 10, 50, 100, 500, 1000]],\n",
    "          \"Lasso\" : [{\"alpha\" : x} for x in [.1, .5, 1, 5, 10, 50, 100, 500, 1000]],\n",
    "          \"ElasticNet\" : [{\"alpha\" : x,\n",
    "                         \"l1_ratio\" : y} \n",
    "                          for x in [.1, 1, 10, 100, 1000] \n",
    "                          for y in [.1, .3, .5, .7, .9]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RMSE': 27.428745039133098,\n",
       "  'MAE': 22.780952380952378,\n",
       "  'r^2': -0.0038345257630534313}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_models = []\n",
    "test_results, best = grid_search.choose_and_test_model(test_df, models, p_grid)\n",
    "top_models.append(test_results)\n",
    "display(top_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT RUN: ONEL'S WORKING EDITS\n",
    "def time_train_test_split(df, n_splits):\n",
    "    \"\"\"\n",
    "    Splits a dataframe into training, validation, and testing sets for time series\n",
    "    machine learning.\n",
    "\n",
    "\tInputs:\n",
    "\tdf(Dataframe): dataframe structure with features for machine learning\n",
    "        \tsplits; must have a 'Year' column or have the index be a datetime object \n",
    "        \tand must have at least three years of data for the splits.\n",
    "\n",
    "        n_splits (int): number of splits by years of data; typically = \n",
    "        \tnumber of years - 2\n",
    "\n",
    "\tOutputs:\n",
    "        splits (dict): a dictionary of key:value pairs with each component\n",
    "\t\trepresenting a sequentially increasing (by years) set of dataframes \n",
    "        to be used as training/validation and testing sets.\n",
    "    \"\"\"\n",
    "\n",
    "    if (n_splits < 3):\n",
    "        raise ValueError(\"Number of splits must be (3) or more.\")\n",
    "\n",
    "    if \"Year\" not in df.columns:\n",
    "        if is_datetime(df.index):\n",
    "            df['Year'] = df.index.year\n",
    "            df = df.reset_index()\n",
    "        else:\n",
    "            raise TypeError(\"Index is not of datetime type to create year column.\")\n",
    "    \n",
    "    year_list = df['Year'].unique().tolist()\n",
    "    splits = {'train': [], 'validation':[], 'test': []}\n",
    "\n",
    "    for idx, yr in enumerate(year_list[:-2]):\n",
    "        train_yr = year_list[:idx+1]\n",
    "        valid_yr = year_list[idx+1:idx+2]\n",
    "        test_yr = [year_list[idx+2]]\n",
    "        print('TRAIN: {}, VALIDATION: {}, TEST: {}'.format(train_yr, valid_yr, test_yr))\n",
    "        \n",
    "        splits['train'].append(df.loc[df.Year.isin(train_yr), :])\n",
    "        splits['validation'].append(df.loc[df.Year.isin(valid_yr), :])\n",
    "        splits['test'].append(df.loc[df.Year.isin(test_yr), :])\n",
    "    return splits\n",
    "\n",
    "test_df = create_test_df()\n",
    "split_df_dict = time_train_test_split(test_df, 3)\n",
    "split_df_dict['test'][0]"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
