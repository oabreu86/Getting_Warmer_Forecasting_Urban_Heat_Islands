{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/geopandas/_compat.py:106: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#Helper Functions for Data processing:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import dask \n",
    "import numba\n",
    "import libpysal as lp\n",
    "from rasterstats import zonal_stats\n",
    "import os\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import json\n",
    "from rasterio.mask import mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_spatial(path, espg_code):\n",
    "    '''\n",
    "    Function to read spatial data and converts to ESPG 3435\n",
    "    Input: path to the data file\n",
    "            epgs_code (string): epsg code as a string\n",
    "    Output: a gpd object \n",
    "    '''\n",
    "    epsg = \"EPSG:\" + espg_code\n",
    "    file=gpd.read_file(path)\n",
    "    file=file.to_crs(epsg)\n",
    "    print(path, file.crs)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET TOY TEST DATA:\n",
    "#com_areas = read_spatial(\"data/com_areas_chi\", \"32616\")\n",
    "#bike_rack_points = pd.read_csv('data/Bike_Racks.csv')\n",
    "#bike_rack_points = bike_rack_points.rename(columns = {\"LOCATION\":\"geometry\"})\n",
    "#bike_racks = gpd.GeoDataFrame(\n",
    "#    bike_rack_points, geometry=gpd.points_from_xy(bike_rack_points.Longitude,\n",
    "#                                                  bike_rack_points.Latitude), \n",
    "#    crs=\"EPSG:4326\")\n",
    "#bike_racks = bike_racks.to_crs(\"EPSG:32616\")\n",
    "#bike_racks = bike_racks[[\"RackID\", \"Address\", \"F12\", \"F13\", \"geometry\"]]\n",
    "#bike_racks[\"F12\"] = bike_racks[\"F12\"].round(1)\n",
    "#bike_racks[\"F13\"] = bike_racks[\"F13\"].round(1)\n",
    "#base = com_areas.plot(color='white', edgecolor='black')\n",
    "#bike_racks.plot(ax=base, marker='o', color='red', markersize=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_point_to_polygon(polygon_data, poly_unique_id, other_file):\n",
    "    '''\n",
    "    A function that creates a new dataset with spatial join of pyolgon data to points\n",
    "    Inputs:\n",
    "        polygon_data (gpd): polygon data (i.e. Community Areas)\n",
    "        poly_unique_id (list of strings): the unique identifiers of the polygon spatial data\n",
    "        other_file (gpd): another point-based gpd. Data from this will be aggregated \n",
    "                          to the polygon scale\n",
    "        agg_dict (dict keys: strings - col names in point data\n",
    "                       values: strings or list of strings - how to aggregate)\n",
    "                       \n",
    "    Output:\n",
    "        the polygon_data dataframe updated with the new column\n",
    "    Note: .size suggestion from here: https://stackoverflow.com/questions/19384532/get-statistics-for-each-group-such-as-count-mean-etc-using-pandas-groupby\n",
    "    '''\n",
    "    spatial_join = gpd.sjoin(other_file, \n",
    "                             polygon_data[poly_unique_id + [\"geometry\"]], \n",
    "                             how=\"inner\", \n",
    "                             op='intersects')\n",
    "    \n",
    "    return spatial_join\n",
    "#USE CASE: \n",
    "#INPUT com areas and point data with all of the ND.. features\n",
    "#We get the com area the point data is in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy Test:\n",
    "#bike_ag_dict = {'RackID': ['min', 'max'], 'Address': 'size'}\n",
    "\n",
    "#com_area_w_bike = conduct_point_to_polygon(com_areas, \n",
    "#                                           [\"community\", \"area_numbe\"], \n",
    "#                                           bike_racks)\n",
    "#com_area_w_bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_cell_data(df, cols_to_group_by, ag_dict):\n",
    "    '''\n",
    "    Function to aggregate point weather data the cols_to_group_by and perform aggregations\n",
    "    as specificed in the aggregation_dictionary\n",
    "    Inputs:\n",
    "        df the dataframe\n",
    "        cols_to_group_by (list of strings): columns in df\n",
    "        ag_dict (dictionary of strings to list of strings): maps column names to \n",
    "                                    aggregation operations\n",
    "    Outputs:\n",
    "        aggregated df\n",
    "    '''\n",
    "    df_new = df.groupby(cols_to_group_by).agg(ag_dict)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "#USE CASE: ONCE WE HAVE PREVIOUS DATA, WE GROUP BY COM AREA, PERIOD, YEAR\n",
    "# THEN WE AGGREGATE: MEAN, MAX OF DIFFERENT ND.. COLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = [\"area_numbe\", \"F12\"]\n",
    "#ag_dict1 = {\"RackID\":\"mean\", \"F13\": [\"min\", \"max\"]}\n",
    "#bike_rack_agg = agg_cell_data(com_area_w_bike, cols_to_group_by = cols, \n",
    "#                             ag_dict = ag_dict1)\n",
    "#bike_rack_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get raster data:\n",
    "\n",
    "#SHARED_DATA_FOLDER = Path('tif_data_2021')\n",
    "\n",
    "#construct SCENE NAME\n",
    "# path_row = []  \n",
    "# year = []\n",
    "# month = []\n",
    "# day = []\n",
    "\n",
    "#SCENE = 'LC08_L2SP_022031_20210514_20210525_02_T1'\n",
    "\n",
    "#check if scene_path is a file\n",
    "\n",
    "#scene_path = SHARED_DATA_FOLDER/SCENE\n",
    "#b1_path =scene_path/\"{}_SR_B2.TIF\".format(SCENE)\n",
    "#b1 = rasterio.open(b1_path)\n",
    "#chi_b = gpd.read_file(\"chi_b\")\n",
    "#chi_b = chi_b.to_crs(\"EPSG:32616\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b1 = rasterio.open(b1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRS.from_epsg(32616)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b1.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_raster(raster, vector_poly):\n",
    "    '''\n",
    "    Clip raster polygon with the vecotr polygon boundary\n",
    "    Input: \n",
    "        raster - rasterio object (i.e. read from rasterio.open)\n",
    "        vector_poly - geopandas dataframe\n",
    "    Output:\n",
    "        raster object that is clipped\n",
    "    '''\n",
    "    vector_poly_good_crs = vector_poly.to_crs(raster.crs)\n",
    "    vector_as_json = [json.loads(vector_poly.to_json())['features'][0]['geometry']]\n",
    "    \n",
    "    out_img, out_transform = mask(dataset=raster, \n",
    "                                  shapes=vector_as_json, \n",
    "                                  crop=True)\n",
    "    return out_img, out_transform\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raster, new_raster_transform = clip_raster(b1, chi_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 5.],\n",
       "       [4., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_max(list_of_arrays):\n",
    "    '''\n",
    "    Function that computes the elementwise max of the arrays in the list of arrays\n",
    "    Inputs:\n",
    "        list_of_arrays (list of 1D np arrays), note the arrays should be the same size\n",
    "    Output:\n",
    "        max_array (1D np array): the elementwise max from each array\n",
    "    '''\n",
    "    n, p = list_of_arrays[0].shape\n",
    "    mx = np.zeros((n, p))  \n",
    "    for i in range(len(list_of_arrays)):\n",
    "        mx = np.maximum(list_of_arrays[i], mx)\n",
    "    \n",
    "    return mx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a = np.array([[1, 2], [4, -1]])    \n",
    "b = np.array([[-1, 5], [0, 1]])\n",
    "c = np.array([[1, 1], [1, 1]])\n",
    "d = np.array([[-1, -1], [-1, -1]])\n",
    "l = [a, b, c, d]\n",
    "\n",
    "\n",
    "compute_max(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid number of arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6962dabe17dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-fe63f9a498c4>\u001b[0m in \u001b[0;36mcompute_max\u001b[0;34m(list_of_arrays)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmax_array\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0mD\u001b[0m \u001b[0mnp\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0melementwise\u001b[0m \u001b[0mmax\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meach\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     '''\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmax_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist_of_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid number of arguments"
     ]
    }
   ],
   "source": [
    "compute_max([a, b, c, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/com_areas_chi EPSG:32616\n"
     ]
    }
   ],
   "source": [
    "#Read and set community areas up: \n",
    "com_areas = read_spatial(\"data/com_areas_chi\", \"32616\")\n",
    "com_areas2 = com_areas[[\"area_numbe\", \"community\", \"geometry\"]]\n",
    "#com_areas3 = com_areas2[com_areas2[:, 0].argsort()]\n",
    "#com_areas2\n",
    "com_areas2[\"number\"] = pd.to_numeric(com_areas2[\"area_numbe\"])\n",
    "com_areas3 = com_areas2.sort_values(by=[\"number\"])\n",
    "com_areas3 #This is the com areas to use\n",
    "\n",
    "com_areas_no_spatial = com_areas3[[\"area_numbe\", \"community\", \"number\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = np.array(test)\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W(com_areas):\n",
    "    '''\n",
    "    Function that takes community areas shapefile (should be sorted by area number)\n",
    "    Input:\n",
    "        com_areas (gpd): a community areas shapefile sorted by area number\n",
    "    Output:\n",
    "        W (2D np.array): the spatial weights matrix\n",
    "    \n",
    "    '''\n",
    "    #Get pysal weights matrix:\n",
    "    weights_matrix = lp.weights.Queen.from_dataframe(com_areas, idVariable='area_numbe')\n",
    "\n",
    "    #Access their dictionary\n",
    "    w_dict = weights_matrix.neighbors\n",
    "    W = np.zeros((77, 77))\n",
    "\n",
    "    for com_area, list_of_neighbors in w_dict.items():\n",
    "        for neighbor in list_of_neighbors:\n",
    "            W[int(com_area) -1, int(neighbor) -1] = 1\n",
    "\n",
    "    #Standardize:\n",
    "    sum_of_rows = W.sum(axis=1)\n",
    "    W = W / sum_of_rows[:, np.newaxis]\n",
    "    \n",
    "    return W\n",
    "\n",
    "#W = get_W(com_areas3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spatial_lag(data_array, W):\n",
    "    '''\n",
    "    Computes the first order queen contiguity spatial lag for data in the data_array given weights matrix W\n",
    "    Inputs:\n",
    "        data_array (2D np.array): data matrix with rows indicating community areas (ordered by com area number)\n",
    "        W (2D np.array): a spatial weights matrix of size 77 x 77\n",
    "    Outputs:\n",
    "        data_array (2D np.array): the data array, but with columns appended for each spatial lag\n",
    "    '''\n",
    "    n, p = data_array.shape\n",
    "    new_data_array = data_array\n",
    "    \n",
    "    for i in range(p):\n",
    "        col = data_array[:,i]\n",
    "        lag = W @ col\n",
    "        lag = lag.reshape(-1, 1)\n",
    "        new_data_array = np.hstack((new_data_array, lag))\n",
    "    return new_data_array \n",
    "\n",
    "out = compute_spatial_lag(test, W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zonal_stats(path_to_raster, vector, band_name):\n",
    "    '''\n",
    "    Inputs: \n",
    "        path_to_raster (string): path to raster data\n",
    "        vector (geopandas df): \n",
    "        band_name (string): name of band\n",
    "    Outputs:\n",
    "        nparray (1D np array): mean values of the raster data \n",
    "                             ordered by com area order\n",
    "    '''\n",
    "    col_name = \"mean_\" + band_name\n",
    "    sum_stats = zonal_stats(vector, file, \n",
    "                            # nodata = Nan,\n",
    "                            stats=[\"mean\"])\n",
    "    print(sum_stats)\n",
    "    df = pd.DataFrame(sum_stats)\n",
    "    print(df)\n",
    "    df = df.rename(columns = {\"mean\": col_name})\n",
    "    nparray = np.array(df)\n",
    "    \n",
    "    return nparray\n",
    "\n",
    "\n",
    "#file = \"../data_2013_2015/LC08_L2SP_022031_20130524_20200913_02_T1/LC08_L2SP_022031_20130524_20200913_02_T1_SR_B1.TIF\"\n",
    "#a = compute_zonal_stats(file, com_areas2, \"b1\")\n",
    "#print(a.shape, a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
