{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/geopandas/_compat.py:106: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#Helper Functions for Data processing:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import dask \n",
    "import numba\n",
    "import libpysal as lp\n",
    "from rasterstats import zonal_stats\n",
    "import os\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import json\n",
    "from rasterio.mask import mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_spatial(path, espg_code):\n",
    "    '''\n",
    "    Function to read spatial data and converts to ESPG 3435\n",
    "    Input: path to the data file\n",
    "            epgs_code (string): epsg code as a string\n",
    "    Output: a gpd object \n",
    "    '''\n",
    "    epsg = \"EPSG:\" + espg_code\n",
    "    file=gpd.read_file(path)\n",
    "    file=file.to_crs(epsg)\n",
    "    print(path, file.crs)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET TOY TEST DATA:\n",
    "#com_areas = read_spatial(\"data/com_areas_chi\", \"32616\")\n",
    "#bike_rack_points = pd.read_csv('data/Bike_Racks.csv')\n",
    "#bike_rack_points = bike_rack_points.rename(columns = {\"LOCATION\":\"geometry\"})\n",
    "#bike_racks = gpd.GeoDataFrame(\n",
    "#    bike_rack_points, geometry=gpd.points_from_xy(bike_rack_points.Longitude,\n",
    "#                                                  bike_rack_points.Latitude), \n",
    "#    crs=\"EPSG:4326\")\n",
    "#bike_racks = bike_racks.to_crs(\"EPSG:32616\")\n",
    "#bike_racks = bike_racks[[\"RackID\", \"Address\", \"F12\", \"F13\", \"geometry\"]]\n",
    "#bike_racks[\"F12\"] = bike_racks[\"F12\"].round(1)\n",
    "#bike_racks[\"F13\"] = bike_racks[\"F13\"].round(1)\n",
    "#base = com_areas.plot(color='white', edgecolor='black')\n",
    "#bike_racks.plot(ax=base, marker='o', color='red', markersize=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_point_to_polygon(polygon_data, poly_unique_id, other_file):\n",
    "    '''\n",
    "    A function that creates a new dataset with spatial join of pyolgon data to points\n",
    "    Inputs:\n",
    "        polygon_data (gpd): polygon data (i.e. Community Areas)\n",
    "        poly_unique_id (list of strings): the unique identifiers of the polygon spatial data\n",
    "        other_file (gpd): another point-based gpd. Data from this will be aggregated \n",
    "                          to the polygon scale\n",
    "        agg_dict (dict keys: strings - col names in point data\n",
    "                       values: strings or list of strings - how to aggregate)\n",
    "                       \n",
    "    Output:\n",
    "        the polygon_data dataframe updated with the new column\n",
    "    Note: .size suggestion from here: https://stackoverflow.com/questions/19384532/get-statistics-for-each-group-such-as-count-mean-etc-using-pandas-groupby\n",
    "    '''\n",
    "    spatial_join = gpd.sjoin(other_file, \n",
    "                             polygon_data[poly_unique_id + [\"geometry\"]], \n",
    "                             how=\"inner\", \n",
    "                             op='intersects')\n",
    "    \n",
    "    return spatial_join\n",
    "#USE CASE: \n",
    "#INPUT com areas and point data with all of the ND.. features\n",
    "#We get the com area the point data is in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toy Test:\n",
    "#bike_ag_dict = {'RackID': ['min', 'max'], 'Address': 'size'}\n",
    "\n",
    "#com_area_w_bike = conduct_point_to_polygon(com_areas, \n",
    "#                                           [\"community\", \"area_numbe\"], \n",
    "#                                           bike_racks)\n",
    "#com_area_w_bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_cell_data(df, cols_to_group_by, ag_dict):\n",
    "    '''\n",
    "    Function to aggregate point weather data the cols_to_group_by and perform aggregations\n",
    "    as specificed in the aggregation_dictionary\n",
    "    Inputs:\n",
    "        df the dataframe\n",
    "        cols_to_group_by (list of strings): columns in df\n",
    "        ag_dict (dictionary of strings to list of strings): maps column names to \n",
    "                                    aggregation operations\n",
    "    Outputs:\n",
    "        aggregated df\n",
    "    '''\n",
    "    df_new = df.groupby(cols_to_group_by).agg(ag_dict)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "#USE CASE: ONCE WE HAVE PREVIOUS DATA, WE GROUP BY COM AREA, PERIOD, YEAR\n",
    "# THEN WE AGGREGATE: MEAN, MAX OF DIFFERENT ND.. COLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = [\"area_numbe\", \"F12\"]\n",
    "#ag_dict1 = {\"RackID\":\"mean\", \"F13\": [\"min\", \"max\"]}\n",
    "#bike_rack_agg = agg_cell_data(com_area_w_bike, cols_to_group_by = cols, \n",
    "#                             ag_dict = ag_dict1)\n",
    "#bike_rack_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get raster data:\n",
    "\n",
    "#SHARED_DATA_FOLDER = Path('tif_data_2021')\n",
    "\n",
    "#construct SCENE NAME\n",
    "# path_row = []  \n",
    "# year = []\n",
    "# month = []\n",
    "# day = []\n",
    "\n",
    "#SCENE = 'LC08_L2SP_022031_20210514_20210525_02_T1'\n",
    "\n",
    "#check if scene_path is a file\n",
    "\n",
    "#scene_path = SHARED_DATA_FOLDER/SCENE\n",
    "#b1_path =scene_path/\"{}_SR_B2.TIF\".format(SCENE)\n",
    "#b1 = rasterio.open(b1_path)\n",
    "#chi_b = gpd.read_file(\"chi_b\")\n",
    "#chi_b = chi_b.to_crs(\"EPSG:32616\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b1 = rasterio.open(b1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRS.from_epsg(32616)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b1.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_raster(raster, vector_poly):\n",
    "    '''\n",
    "    Clip raster polygon with the vecotr polygon boundary\n",
    "    Input: \n",
    "        raster - rasterio object (i.e. read from rasterio.open)\n",
    "        vector_poly - geopandas dataframe\n",
    "    Output:\n",
    "        raster object that is clipped\n",
    "    '''\n",
    "    vector_poly_good_crs = vector_poly.to_crs(raster.crs)\n",
    "    vector_as_json = [json.loads(vector_poly.to_json())['features'][0]['geometry']]\n",
    "    \n",
    "    out_img, out_transform = mask(dataset=raster, \n",
    "                                  shapes=vector_as_json, \n",
    "                                  crop=True)\n",
    "    return out_img, out_transform\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raster, new_raster_transform = clip_raster(b1, chi_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max(list_of_arrays):\n",
    "    '''\n",
    "    Function that computes the elementwise max of the arrays in the list of arrays\n",
    "    Inputs:\n",
    "        list_of_arrays (list of 1D np arrays), note the arrays should be the same size\n",
    "    Output:\n",
    "        max_array (1D np array): the elementwise max from each array\n",
    "    '''\n",
    "    n, p = list_of_arrays[0].shape\n",
    "    mx = np.zeros((n, p))  \n",
    "    for a in list_of_arrays:\n",
    "        mx = np.maximum(a, mx)\n",
    "    \n",
    "    return mx\n",
    "\n",
    "#a = np.array([[1, 2], [4, -1]])    \n",
    "#b = np.array([[-1, 5], [0, 1]])\n",
    "#c = np.array([[1, 1], [1, 1]])\n",
    "#d = np.array([[-1, -1], [-1, -1]])\n",
    "#l = [a, b, c, d, a, b, c, d, a]\n",
    "#compute_max(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 5.],\n",
       "       [4., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_max([a, b, c, d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/com_areas_chi EPSG:32616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/geopandas/geodataframe.py:1322: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super(GeoDataFrame, self).__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "#Read and set community areas up: \n",
    "com_areas = read_spatial(\"data/com_areas_chi\", \"32616\")\n",
    "com_areas2 = com_areas[[\"area_numbe\", \"community\", \"geometry\"]]\n",
    "#com_areas3 = com_areas2[com_areas2[:, 0].argsort()]\n",
    "#com_areas2\n",
    "com_areas2[\"number\"] = pd.to_numeric(com_areas2[\"area_numbe\"])\n",
    "com_areas3 = com_areas2.sort_values(by=[\"number\"])\n",
    "com_areas3 #This is the com areas to use\n",
    "\n",
    "com_areas_no_spatial = com_areas3[[\"area_numbe\", \"community\", \"number\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = np.array(test)\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W(com_areas):\n",
    "    '''\n",
    "    Function that takes community areas shapefile (should be sorted by area number)\n",
    "    Input:\n",
    "        com_areas (gpd): a community areas shapefile sorted by area number\n",
    "    Output:\n",
    "        W (2D np.array): the spatial weights matrix\n",
    "    \n",
    "    '''\n",
    "    #Get pysal weights matrix:\n",
    "    weights_matrix = lp.weights.Queen.from_dataframe(com_areas, idVariable='area_numbe')\n",
    "\n",
    "    #Access their dictionary\n",
    "    w_dict = weights_matrix.neighbors\n",
    "    W = np.zeros((77, 77))\n",
    "\n",
    "    for com_area, list_of_neighbors in w_dict.items():\n",
    "        for neighbor in list_of_neighbors:\n",
    "            W[int(com_area) -1, int(neighbor) -1] = 1\n",
    "\n",
    "    #Standardize:\n",
    "    sum_of_rows = W.sum(axis=1)\n",
    "    W = W / sum_of_rows[:, np.newaxis]\n",
    "    \n",
    "    return W\n",
    "\n",
    "#W = get_W(com_areas3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spatial_lag(data_array, W):\n",
    "    '''\n",
    "    Computes the first order queen contiguity spatial lag for data in the data_array given weights matrix W\n",
    "    Inputs:\n",
    "        data_array (2D np.array): data matrix with rows indicating community areas (ordered by com area number)\n",
    "        W (2D np.array): a spatial weights matrix of size 77 x 77\n",
    "    Outputs:\n",
    "        data_array (2D np.array): the data array, but with columns appended for each spatial lag\n",
    "    '''\n",
    "    n, p = data_array.shape\n",
    "    new_data_array = data_array\n",
    "    \n",
    "    for i in range(p):\n",
    "        col = data_array[:,i]\n",
    "        lag = W @ col\n",
    "        lag = lag.reshape(-1, 1)\n",
    "        new_data_array = np.hstack((new_data_array, lag))\n",
    "    return new_data_array \n",
    "\n",
    "out = compute_spatial_lag(test, W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask_building_data.ipynb\r\n",
      "\u001b[34mLC08_L2SP_023031_20130904_20200912_02_T1\u001b[m\u001b[m\r\n",
      "LC08_L2SP_023031_20130904_20200912_02_T1.tar\r\n",
      "Land_surface_temp_calulator.py\r\n",
      "Project_gdm.ipynb\r\n",
      "Project_gdm1.ipynb\r\n",
      "README.md\r\n",
      "Untitled.ipynb\r\n",
      "\u001b[34mchi_b\u001b[m\u001b[m\r\n",
      "chi_b.cpg\r\n",
      "chi_b.dbf\r\n",
      "chi_b.prj\r\n",
      "chi_b.qpj\r\n",
      "chi_b.shp\r\n",
      "chi_b.shx\r\n",
      "\u001b[31mchi_b.zip\u001b[m\u001b[m\r\n",
      "\u001b[34mdata\u001b[m\u001b[m\r\n",
      "get_landsat_data.py\r\n",
      "get_landsat_data.sbatch\r\n",
      "time_series_split.py\r\n",
      "util.py\r\n",
      "utils.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7951, 7841)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.20772682],\n",
       "       [0.19564331],\n",
       "       [0.2089464 ],\n",
       "       [0.1861174 ],\n",
       "       [0.17212766],\n",
       "       [0.18262329],\n",
       "       [0.17880583],\n",
       "       [0.17063834],\n",
       "       [0.1775142 ],\n",
       "       [0.18189693],\n",
       "       [0.18034152],\n",
       "       [0.16393084],\n",
       "       [0.18782113],\n",
       "       [0.18138435],\n",
       "       [0.17451679],\n",
       "       [0.17785677],\n",
       "       [0.18202376],\n",
       "       [0.17721729],\n",
       "       [0.18172259],\n",
       "       [0.1793875 ],\n",
       "       [0.19032342],\n",
       "       [0.18364333],\n",
       "       [0.19223613],\n",
       "       [0.19295021],\n",
       "       [0.18443179],\n",
       "       [0.19599144],\n",
       "       [0.20545481],\n",
       "       [0.20723008],\n",
       "       [0.20627781],\n",
       "       [0.19790727],\n",
       "       [0.20395322],\n",
       "       [0.15756092],\n",
       "       [0.22431137],\n",
       "       [0.20121338],\n",
       "       [0.20923171],\n",
       "       [0.21381369],\n",
       "       [0.20723384],\n",
       "       [0.2093681 ],\n",
       "       [0.18655243],\n",
       "       [0.20653025],\n",
       "       [0.18892075],\n",
       "       [0.19486871],\n",
       "       [0.1875129 ],\n",
       "       [0.17612655],\n",
       "       [0.16781336],\n",
       "       [0.17223131],\n",
       "       [0.18019996],\n",
       "       [0.16872442],\n",
       "       [0.18034148],\n",
       "       [0.20876547],\n",
       "       [0.15696684],\n",
       "       [0.16760895],\n",
       "       [0.17260428],\n",
       "       [0.17703135],\n",
       "       [0.15135861],\n",
       "       [0.19776111],\n",
       "       [0.22821358],\n",
       "       [0.19499797],\n",
       "       [0.1955854 ],\n",
       "       [0.19310448],\n",
       "       [0.19786597],\n",
       "       [0.18394349],\n",
       "       [0.18223727],\n",
       "       [0.20522877],\n",
       "       [0.19681075],\n",
       "       [0.17978503],\n",
       "       [0.17442742],\n",
       "       [0.18236871],\n",
       "       [0.18705864],\n",
       "       [0.19605164],\n",
       "       [0.18075512],\n",
       "       [0.15879754],\n",
       "       [0.17848376],\n",
       "       [0.18880858],\n",
       "       [0.17593709],\n",
       "       [0.25213709],\n",
       "       [0.19813622]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_zonal_stats(path_to_raster, vector, band_name):\n",
    "    '''\n",
    "    This function first converts raw bands into their acceptable ranges (see page 12 of\n",
    "    the link below for more details) and then computes the zonal stats of the for the \n",
    "    inputted band for community areas. s\n",
    "    \n",
    "    Inputs: \n",
    "        path_to_raster (string): path to raster data\n",
    "        vector (geopandas df): \n",
    "        band_name (string): name of band\n",
    "    Outputs:\n",
    "        nparray (1D np array): mean values of the raster data \n",
    "                             ordered by com area order\n",
    "    '''\n",
    "    col_name = \"mean_\" + band_name\n",
    "    \n",
    "    raster = rasterio.open(path_to_raster)\n",
    "    affine = raster.transform\n",
    "    raster = raster.read()[0]\n",
    "    \n",
    "    print(raster.shape)\n",
    "    if band_name in [\"b1\", \"b2\", \"b3\", \"b4\", \"b5\", \"b7\"]:\n",
    "        raster = np.where((raster > 7273) & (raster < 43537), raster, 0)\n",
    "        raster = (raster * 0.0000275) -0.2\n",
    "    elif band_name == \"b6\":\n",
    "        raster = (raster * 0.0000275) -0.2\n",
    "    else: #band_name = \"b10\"\n",
    "        raster = (raster * 0.00341802) + 149\n",
    "    \n",
    "    \n",
    "    sum_stats = zonal_stats(vector, raster, \n",
    "                            nodata = -999,\n",
    "                            affine = affine,\n",
    "                            stats=[\"mean\"])\n",
    "    df = pd.DataFrame(sum_stats)\n",
    "    df = df.rename(columns = {\"mean\": col_name})\n",
    "    nparray = np.array(df)\n",
    "    \n",
    "    return nparray\n",
    "\n",
    "path = \"LC08_L2SP_023031_20130904_20200912_02_T1/LC08_L2SP_023031_20130904_20200912_02_T1_SR_B6.TIF\"\n",
    "a = compute_zonal_stats(path, com_areas3, \"b6\")\n",
    "#file = \"../data_2013_2015/LC08_L2SP_022031_20130524_20200913_02_T1/LC08_L2SP_022031_20130524_20200913_02_T1_SR_B1.TIF\"\n",
    "#print(a.shape, a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
